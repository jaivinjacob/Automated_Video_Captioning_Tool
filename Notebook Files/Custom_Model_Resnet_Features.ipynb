{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028b7228-9ad6-4c08-bfa0-5f5fe767bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8339a8ed-1eb2-4e14-98c6-20d93e442f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('filtered_videos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5632d5-fa4c-46ed-9a8c-cf2484fbe025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21179416</td>\n",
       "      <td>videos_1000\\21179416.mp4</td>\n",
       "      <td>Aerial shot winter forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5629184</td>\n",
       "      <td>videos_1000\\5629184.mp4</td>\n",
       "      <td>Senior couple looking through binoculars on sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1063125190</td>\n",
       "      <td>videos_1000\\1063125190.mp4</td>\n",
       "      <td>A beautiful cookie with oranges lies on a gree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1039695998</td>\n",
       "      <td>videos_1000\\1039695998.mp4</td>\n",
       "      <td>Japanese highrise office skyscrapers tokyo square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9607838</td>\n",
       "      <td>videos_1000\\9607838.mp4</td>\n",
       "      <td>Zrenjanin,serbia march 21 2015: fans watching ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     video_id                  video_path  \\\n",
       "0    21179416    videos_1000\\21179416.mp4   \n",
       "1     5629184     videos_1000\\5629184.mp4   \n",
       "2  1063125190  videos_1000\\1063125190.mp4   \n",
       "3  1039695998  videos_1000\\1039695998.mp4   \n",
       "4     9607838     videos_1000\\9607838.mp4   \n",
       "\n",
       "                                         description  \n",
       "0                          Aerial shot winter forest  \n",
       "1  Senior couple looking through binoculars on sa...  \n",
       "2  A beautiful cookie with oranges lies on a gree...  \n",
       "3  Japanese highrise office skyscrapers tokyo square  \n",
       "4  Zrenjanin,serbia march 21 2015: fans watching ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7117bab1-1a72-4d77-a909-07cb770c145e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id       999\n",
       "video_path     999\n",
       "description    999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da360227-91ee-482b-9bd6-91f1a51ec369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video features\n",
    "video_features = np.load('video_features_pytorch_new.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ea7b99-e17b-4252-8a25-5618be545fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract descriptions\n",
    "descriptions = df['description'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71fa4aa1-9dbe-45fc-8f3c-282ba6afc603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senior couple looking through binoculars on sailboat together. shot on red epic for high quality 4k, uhd, ultra hd resolution.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f2e0927-c99e-4f6a-90f2-0d7378f2399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the descriptions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8203d6ac-bb4d-401b-b473-04be8631f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert descriptions to sequences\n",
    "sequences = tokenizer.texts_to_sequences(descriptions)\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a096d736-8695-488a-baa3-96f6f9ebe0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the video features\n",
    "video_ids = df['video_id'].values\n",
    "X_video = np.array([video_features[str(video_id)] for video_id in video_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ac993e7-a502-4062-a192-f8c157cbcbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_video = X_video.squeeze() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33085c9b-2c22-490e-9eb8-3ac49b2325c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the target sequences\n",
    "y = np.expand_dims(padded_sequences, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa996225-a5e7-4ba2-9f04-56a9ece587ce",
   "metadata": {},
   "source": [
    "Define the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aa7e4f9-fd2e-4377-bfae-82060109c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, RepeatVector, TimeDistributed, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "202fb5dd-44d4-418d-a388-dc6bbd7069e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder\n",
    "video_input = Input(shape=(2048,))\n",
    "encoder = Dense(256, activation='relu')(video_input)\n",
    "encoder = RepeatVector(max_length)(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45c01d81-92b9-461f-b795-5415deb87283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder\n",
    "decoder_input = Input(shape=(max_length,))\n",
    "decoder_embedding = Embedding(vocab_size, 256, mask_zero=True)(decoder_input)\n",
    "decoder_lstm = LSTM(256, return_sequences=True)(decoder_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ced3a66c-fcdc-4ce9-9214-c24132ff1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mechanism\n",
    "attention = Attention()([encoder, decoder_lstm])\n",
    "decoder_combined_context = tf.concat([decoder_lstm, attention], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "869180b6-75bc-41f4-85b1-e7c37aace314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeDistributed layer for word prediction\n",
    "decoder_output = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder_combined_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec669f09-4cc0-41c9-933b-5aa84b5fd612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model(inputs=[video_input, decoder_input], outputs=decoder_output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e2a3337-e745-4e9e-87e8-d8f5879e333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 37)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 37, 256)      1001216     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 37, 256)      525312      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 37, 256)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 37, 256)      0           repeat_vector[0][0]              \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 37, 512)      0           lstm[0][0]                       \n",
      "                                                                 attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 37, 3911)     2006343     tf.concat[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,057,415\n",
      "Trainable params: 4,057,415\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2cda28-b532-405d-82f5-87a1effa8f76",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02981caf-eba3-46ea-8400-2c1ab55a992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - 19s 127ms/step - loss: 6.2585 - val_loss: 3.8930\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 3.3141 - val_loss: 3.5067\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 2.9558 - val_loss: 3.3770\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.8460 - val_loss: 3.3236\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.7991 - val_loss: 3.2892\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.7609 - val_loss: 3.3110\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 2.7255 - val_loss: 3.3399\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.6868 - val_loss: 3.3246\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 2.6340 - val_loss: 3.3439\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 2.5774 - val_loss: 3.2764\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.5301 - val_loss: 3.2233\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.4892 - val_loss: 3.1993\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.4486 - val_loss: 3.2077\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.4210 - val_loss: 3.2062\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.3840 - val_loss: 3.1845\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.3481 - val_loss: 3.1595\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.3098 - val_loss: 3.1365\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.2693 - val_loss: 3.1260\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.2304 - val_loss: 3.1036\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.1880 - val_loss: 3.0871\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 2.1464 - val_loss: 3.0685\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 2.1066 - val_loss: 3.0503\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 2.0677 - val_loss: 3.0045\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 2.0262 - val_loss: 2.9788\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 1.9830 - val_loss: 2.9667\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 1.9386 - val_loss: 2.9432\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 1.8956 - val_loss: 2.9170\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 1.8510 - val_loss: 2.8993\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 1.8034 - val_loss: 2.8697\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 1.7518 - val_loss: 2.8497\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 1.7058 - val_loss: 2.8096\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 1.6557 - val_loss: 2.7783\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 1.6011 - val_loss: 2.7502\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 1.5493 - val_loss: 2.7218\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 1.4965 - val_loss: 2.6933\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 1.4418 - val_loss: 2.6588\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 1.3952 - val_loss: 2.6365\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 1.3393 - val_loss: 2.6020\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 1.2843 - val_loss: 2.5763\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 1.2440 - val_loss: 2.5225\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 1.1827 - val_loss: 2.5048\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 1.1318 - val_loss: 2.4770\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 1.0722 - val_loss: 2.4330\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 1.0166 - val_loss: 2.3933\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.9728 - val_loss: 2.3848\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.9236 - val_loss: 2.3329\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.8621 - val_loss: 2.3160\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.8094 - val_loss: 2.2919\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.7578 - val_loss: 2.2704\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.7135 - val_loss: 2.2521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x281bdc3ae80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit([X_video, padded_sequences], y, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccb1c7b9-f071-4310-8f25-08ef53da63d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No description generated.\n"
     ]
    }
   ],
   "source": [
    "def generate_description(model, video_feature, tokenizer, max_length):\n",
    "    input_seq = np.zeros((1, max_length))\n",
    "    generated_desc = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        output = model.predict([video_feature, input_seq])\n",
    "        predicted_word_index = np.argmax(output[0, i, :])\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "\n",
    "        if predicted_word == '':\n",
    "            break\n",
    "\n",
    "        generated_desc.append(predicted_word)\n",
    "        input_seq[0, i] = predicted_word_index\n",
    "\n",
    "    return ' '.join(generated_desc)\n",
    "\n",
    "# Example: Generate description for an actual video feature\n",
    "example_video_id = '5629184'  # Replace with any video ID from the dataset\n",
    "example_video_feature = np.expand_dims(video_features[example_video_id].squeeze(), axis=0)\n",
    "description1 = generate_description(model, example_video_feature, tokenizer, max_length)\n",
    "if description1:\n",
    "    print(description1)\n",
    "else:\n",
    "    print(\"No description generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0172a-d29f-4fba-bad3-bdd674ad2d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
