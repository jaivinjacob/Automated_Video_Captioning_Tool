{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b1da93-8f72-4742-b972-72ea6972a8d4",
   "metadata": {},
   "source": [
    "## Generating Text Description\n",
    "\n",
    "Author: Jaivin Jacob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b190e-d5b0-4200-b15f-5305c5090104",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers opencv-python pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6197db-98b1-41a1-b059-a50ea004d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ca8c49-7295-48e8-824c-ca57037efb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load CSV file\n",
    "# df = pd.read_csv('movie_description.csv')\n",
    "\n",
    "# Function to extract frames from video\n",
    "def extract_frames(video_path, num_frames=16):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = max(total_frames // num_frames, 1)\n",
    "    for i in range(0, total_frames, interval):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(cv2.resize(frame, (224, 224)))\n",
    "        if len(frames) == num_frames:\n",
    "            break\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246b376b-44e4-4759-8efb-0dcdd1d31d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb46d6b8-965a-4f20-bd8f-ebe608553119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate captions\n",
    "def generate_caption(model, feature_extractor, tokenizer, frames):\n",
    "    inputs = feature_extractor(images=frames, return_tensors=\"pt\")\n",
    "    pixel_values = inputs.pixel_values\n",
    "    output_ids = model.generate(pixel_values)\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d251c5a4-1493-405f-b4f2-49e11258c304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a dirt road with a few trees and a few cars \n"
     ]
    }
   ],
   "source": [
    "# Function to generate caption for a specific video\n",
    "def generate_caption_for_video(video_path, model, feature_extractor, tokenizer):\n",
    "    frames = extract_frames(video_path)\n",
    "    caption = generate_caption(model, feature_extractor, tokenizer, frames)\n",
    "    return caption\n",
    "\n",
    "# Example usage:\n",
    "video_path = 'sample_video_1.mp4'  # Specify the path to the video\n",
    "caption = generate_caption_for_video(video_path, model, feature_extractor, tokenizer)\n",
    "print(f\"Generated Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2b076-677f-4ccc-be0a-6f417fea6b15",
   "metadata": {},
   "source": [
    "## Generating Auido From Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7017fe4-dbc7-4071-8d7f-c9c8795d52f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f80209-0771-4791-923c-4ae311eaf99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install moviepy pydub speechrecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "877a3b42-26be-4c3c-b3c2-1de26b9b3960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a dirt road with a few trees and a few cars '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b3e0e75-d4e3-4c50-a1cb-25f3e0666159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# Language in which you want to convert\n",
    "language = 'en'\n",
    "\n",
    "# Creating an instance of gTTS\n",
    "speech = gTTS(text=caption, lang=language, slow=False)\n",
    "\n",
    "# Saving the converted audio in a mp3 file named \"caption.mp3\"\n",
    "speech.save(\"caption.mp3\")\n",
    "\n",
    "# # Playing the converted file \n",
    "# os.system(\"start caption.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de1f42c-d43c-436e-afa4-1d684d964f95",
   "metadata": {},
   "source": [
    "## Placing Audio Description on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e0c7a4c-9688-4b7b-afe0-e72375a25ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip\n",
    "from pydub import AudioSegment, silence\n",
    "import moviepy.video.fx.all as vfx\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b30c41-0770-4caf-ad60-a247eb9eac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the video and its audio\n",
    "video = VideoFileClip(\"sample_video_1.mp4\")\n",
    "audio = AudioSegment.from_file(\"sample_video_1.mp4\", \"mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce125362-950e-4d12-a634-266cd9ed948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Find silent parts in the audio\n",
    "silent_ranges = silence.detect_silence(audio, min_silence_len=1000, silence_thresh=audio.dBFS-16)\n",
    "silent_ranges = [((start/1000), (stop/1000)) for start, stop in silent_ranges] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3e0b65-e7f3-4ec3-99f5-c7261c28adcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent part found from 0.0 to 3.377 seconds\n",
      "Moviepy - Building video final_video_with_caption.mp4.\n",
      "MoviePy - Writing audio in final_video_with_captionTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video final_video_with_caption.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready final_video_with_caption.mp4\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Insert the audio description at the first silent part (if available)\n",
    "if silent_ranges:\n",
    "    start_silent, end_silent = silent_ranges[0]\n",
    "    print(f\"Silent part found from {start_silent} to {end_silent} seconds\")\n",
    "\n",
    "    # Step 5: Load the audio description\n",
    "    caption_audio = AudioFileClip(\"caption.mp3\")\n",
    "\n",
    "    # Make sure the audio fits within the silent part\n",
    "    duration = min(caption_audio.duration, end_silent - start_silent)\n",
    "\n",
    "    # Trim or extend the caption audio to fit the silent part\n",
    "    caption_audio = caption_audio.subclip(0, duration)\n",
    "\n",
    "    # Step 6: Split the video into three parts: before, silent part, and after\n",
    "    video_before = video.subclip(0, start_silent)\n",
    "    video_silent = video.subclip(start_silent, start_silent + duration)\n",
    "    video_after = video.subclip(start_silent + duration)\n",
    "\n",
    "    # Replace the silent part's audio with the caption audio\n",
    "    video_silent = video_silent.set_audio(caption_audio)\n",
    "\n",
    "    # Concatenate the video parts back together\n",
    "    final_video = concatenate_videoclips([video_before, video_silent, video_after])\n",
    "\n",
    "    # Step 7: Save the final video\n",
    "    final_video.write_videofile(\"final_video_with_caption.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
    "else:\n",
    "    print(\"No silent part found in the video.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0dfa43-a517-4e77-b762-8def425a8d1a",
   "metadata": {},
   "source": [
    "## Handling Videos With Background Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ad01044-ca88-4859-afb2-fba83675bf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \n",
      "\u001b[A                                                             \n",
      "\n",
      "chunk:  91%|█████████▏| 21/23 [14:21<01:22, 41.00s/it, now=None]\n",
      "chunk:  83%|████████▎ | 19/23 [12:45<02:41, 40.28s/it, now=None]\u001b[A\n",
      "\n",
      "chunk:  78%|███████▊  | 18/23 [11:36<03:13, 38.68s/it, now=None]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "chunk:   0%|          | 0/241 [00:00<?, ?it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                A\n",
      "\u001b[A                                                             \n",
      "\n",
      "chunk:  91%|█████████▏| 21/23 [14:21<01:22, 41.01s/it, now=None]\n",
      "chunk:  83%|████████▎ | 19/23 [12:45<02:41, 40.29s/it, now=None]\u001b[A\n",
      "\n",
      "chunk:  78%|███████▊  | 18/23 [11:36<03:13, 38.69s/it, now=None]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "No speech detected at 0.0s to 1.0s\n",
      "No speech detected at 1.0s to 2.0s\n",
      "No speech detected at 2.0s to 3.0s\n",
      "No speech detected at 3.0s to 4.0s\n",
      "Speech detected: 'it's awesome' at 4.0s to 5.0s\n",
      "No speech detected at 5.0s to 6.0s\n",
      "No speech detected at 6.0s to 7.0s\n",
      "No speech detected at 7.0s to 8.0s\n",
      "No speech detected at 8.0s to 9.0s\n",
      "No speech detected at 9.0s to 10.0s\n",
      "No speech detected at 10.0s to 11.0s\n",
      "Non-speech ranges detected: [(0.0, 4.0), (5.0, 11.0)]\n",
      "Inserting audio description into non-speech part from 0.0 to 4.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \n",
      "\u001b[A                                                             \n",
      "\n",
      "chunk:  91%|█████████▏| 21/23 [14:27<01:22, 41.32s/it, now=None]\n",
      "chunk:  83%|████████▎ | 19/23 [12:52<02:42, 40.64s/it, now=None]\u001b[A\n",
      "\n",
      "                                                                \u001b[A\u001b[A\n",
      "\u001b[A                                                             \n",
      "\n",
      "chunk:  91%|█████████▏| 21/23 [14:27<01:22, 41.32s/it, now=None]\n",
      "chunk:  83%|████████▎ | 19/23 [12:52<02:42, 40.64s/it, now=None]\u001b[A\n",
      "\n",
      "chunk:  78%|███████▊  | 18/23 [11:43<03:15, 39.06s/it, now=None]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video final_video_with_caption.mp4.\n",
      "MoviePy - Writing audio in final_video_with_captionTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "chunk:   0%|          | 0/241 [00:00<?, ?it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "chunk:  12%|█▏        | 28/241 [00:00<00:00, 269.86it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "chunk:  31%|███       | 75/241 [00:00<00:00, 357.50it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                  \u001b[A\u001b[A\u001b[A\n",
      "\u001b[A                                                             \n",
      "\n",
      "chunk:  91%|█████████▏| 21/23 [14:28<01:22, 41.34s/it, now=None]\n",
      "chunk:  83%|████████▎ | 19/23 [12:52<02:42, 40.65s/it, now=None]\u001b[A\n",
      "\n",
      "                                                                \u001b[A\u001b[A\n",
      "\u001b[A                                                             \n",
      "\n",
      "chunk:  91%|█████████▏| 21/23 [14:28<01:22, 41.34s/it, now=None]\n",
      "chunk:  83%|████████▎ | 19/23 [12:52<02:42, 40.65s/it, now=None]\u001b[A\n",
      "\n",
      "chunk:  78%|███████▊  | 18/23 [11:43<03:15, 39.08s/it, now=None]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video final_video_with_caption.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "t:   0%|          | 0/328 [00:00<?, ?it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:   2%|▏         | 7/328 [00:00<00:05, 63.81it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:   5%|▍         | 15/328 [00:00<00:04, 72.26it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:   7%|▋         | 24/328 [00:00<00:03, 77.49it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  10%|▉         | 32/328 [00:00<00:03, 77.06it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  12%|█▏        | 40/328 [00:00<00:04, 71.68it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  15%|█▍        | 48/328 [00:00<00:04, 65.90it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  17%|█▋        | 55/328 [00:00<00:04, 61.53it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  19%|█▉        | 62/328 [00:00<00:04, 54.06it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  21%|██        | 68/328 [00:01<00:05, 51.27it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  23%|██▎       | 74/328 [00:01<00:05, 45.42it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  24%|██▍       | 79/328 [00:01<00:05, 44.46it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  26%|██▌       | 84/328 [00:01<00:05, 44.44it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  27%|██▋       | 89/328 [00:01<00:05, 43.99it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  29%|██▊       | 94/328 [00:01<00:05, 42.94it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  30%|███       | 99/328 [00:02<00:07, 30.16it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  31%|███▏      | 103/328 [00:02<00:07, 31.02it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  33%|███▎      | 107/328 [00:02<00:07, 30.43it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  34%|███▍      | 112/328 [00:02<00:06, 33.30it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  36%|███▌      | 117/328 [00:02<00:05, 35.70it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  37%|███▋      | 122/328 [00:02<00:05, 37.72it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  39%|███▊      | 127/328 [00:02<00:05, 38.33it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  40%|███▉      | 131/328 [00:02<00:06, 32.44it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  41%|████      | 135/328 [00:03<00:06, 30.18it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  42%|████▏     | 139/328 [00:03<00:06, 29.49it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  44%|████▎     | 143/328 [00:03<00:06, 29.19it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  45%|████▌     | 148/328 [00:03<00:05, 32.63it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  47%|████▋     | 153/328 [00:03<00:04, 35.78it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  48%|████▊     | 158/328 [00:03<00:04, 37.40it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  49%|████▉     | 162/328 [00:03<00:04, 37.14it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  51%|█████     | 167/328 [00:03<00:04, 38.81it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  52%|█████▏    | 172/328 [00:04<00:03, 40.00it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  54%|█████▍    | 177/328 [00:04<00:03, 41.36it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  55%|█████▌    | 182/328 [00:04<00:04, 30.95it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  57%|█████▋    | 186/328 [00:04<00:04, 31.62it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  58%|█████▊    | 191/328 [00:04<00:04, 34.04it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  59%|█████▉    | 195/328 [00:04<00:04, 31.08it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  61%|██████    | 200/328 [00:05<00:03, 32.53it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  62%|██████▏   | 204/328 [00:05<00:03, 33.65it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  63%|██████▎   | 208/328 [00:05<00:03, 32.53it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  65%|██████▍   | 212/328 [00:05<00:04, 26.41it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  66%|██████▌   | 216/328 [00:05<00:04, 27.66it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  67%|██████▋   | 219/328 [00:05<00:03, 27.89it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  68%|██████▊   | 223/328 [00:05<00:03, 28.81it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  69%|██████▉   | 226/328 [00:05<00:03, 27.97it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  70%|███████   | 230/328 [00:06<00:03, 30.02it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  71%|███████▏  | 234/328 [00:06<00:02, 31.47it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  73%|███████▎  | 238/328 [00:06<00:02, 33.47it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  74%|███████▍  | 242/328 [00:06<00:02, 34.24it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  75%|███████▌  | 246/328 [00:06<00:02, 34.16it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  76%|███████▌  | 250/328 [00:06<00:02, 35.20it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  77%|███████▋  | 254/328 [00:06<00:02, 36.16it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  79%|███████▊  | 258/328 [00:06<00:01, 36.55it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  80%|████████  | 263/328 [00:06<00:01, 38.12it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  81%|████████▏ | 267/328 [00:07<00:01, 38.46it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  83%|████████▎ | 271/328 [00:07<00:01, 37.96it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  84%|████████▍ | 276/328 [00:07<00:01, 39.29it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  85%|████████▌ | 280/328 [00:07<00:01, 36.86it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  87%|████████▋ | 284/328 [00:07<00:01, 37.14it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  88%|████████▊ | 288/328 [00:07<00:01, 37.14it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  89%|████████▉ | 292/328 [00:07<00:00, 37.54it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  90%|█████████ | 296/328 [00:07<00:00, 35.29it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  91%|█████████▏| 300/328 [00:07<00:00, 35.35it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  93%|█████████▎| 304/328 [00:08<00:00, 36.15it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  94%|█████████▍| 308/328 [00:08<00:00, 33.28it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  95%|█████████▌| 312/328 [00:08<00:00, 34.70it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  96%|█████████▋| 316/328 [00:08<00:00, 33.20it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  98%|█████████▊| 320/328 [00:08<00:00, 33.94it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t:  99%|█████████▉| 324/328 [00:08<00:00, 35.21it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t: 100%|██████████| 328/328 [00:08<00:00, 34.47it/s, now=None]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                A\u001b[A\u001b[A\n",
      "\u001b[A                                                             \n",
      "\n",
      "chunk:  91%|█████████▏| 21/23 [14:38<01:23, 41.83s/it, now=None]\n",
      "chunk:  83%|████████▎ | 19/23 [13:02<02:44, 41.20s/it, now=None]\u001b[A\n",
      "\n",
      "                                                                \u001b[A\u001b[A\n",
      "\u001b[A                                                             \n",
      "\n",
      "chunk:  91%|█████████▏| 21/23 [14:38<01:23, 41.83s/it, now=None]\n",
      "chunk:  83%|████████▎ | 19/23 [13:02<02:44, 41.20s/it, now=None]\u001b[A\n",
      "\n",
      "chunk:  78%|███████▊  | 18/23 [11:53<03:18, 39.65s/it, now=None]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready final_video_with_caption.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "from gtts import gTTS\n",
    "\n",
    "# Ensure ffmpeg is installed and available in the system PATH\n",
    "assert os.system('ffmpeg -version') == 0, \"FFmpeg not found, please install and add it to the system PATH\"\n",
    "\n",
    "# Load the video and extract audio\n",
    "video_path = \"sample_video_1.mp4\"\n",
    "video = VideoFileClip(video_path)\n",
    "audio_path = \"temp_audio.wav\"\n",
    "video.audio.write_audiofile(audio_path)\n",
    "\n",
    "# Load the audio using pydub\n",
    "audio = AudioSegment.from_file(audio_path, format=\"wav\")\n",
    "\n",
    "# Initialize the recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Define the step (chunk size) in milliseconds\n",
    "step = 1000  # 1 second\n",
    "\n",
    "# List to store non-speech ranges\n",
    "non_speech_ranges = []\n",
    "\n",
    "# Loop through the audio file in chunks\n",
    "for i in range(0, len(audio), step):\n",
    "    chunk = audio[i:i + step]\n",
    "    chunk_path = f\"chunk_{i // step}.wav\"\n",
    "    chunk.export(chunk_path, format=\"wav\")\n",
    "    \n",
    "    # Recognize speech in the chunk\n",
    "    with sr.AudioFile(chunk_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            print(f\"Speech detected: '{text}' at {i / 1000}s to {(i + step) / 1000}s\")\n",
    "        except sr.UnknownValueError:\n",
    "            print(f\"No speech detected at {i / 1000}s to {(i + step) / 1000}s\")\n",
    "            non_speech_ranges.append((i / 1000, (i + step) / 1000))\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results; {e}\")\n",
    "    \n",
    "    # Clean up the chunk file\n",
    "    os.remove(chunk_path)\n",
    "\n",
    "# Clean up the extracted audio file\n",
    "os.remove(audio_path)\n",
    "\n",
    "# Combine consecutive non-speech ranges if they are continuous\n",
    "combined_non_speech_ranges = []\n",
    "current_range = None\n",
    "\n",
    "for start, end in non_speech_ranges:\n",
    "    if current_range is None:\n",
    "        current_range = [start, end]\n",
    "    else:\n",
    "        if start == current_range[1]:\n",
    "            current_range[1] = end\n",
    "        else:\n",
    "            combined_non_speech_ranges.append(tuple(current_range))\n",
    "            current_range = [start, end]\n",
    "\n",
    "if current_range is not None:\n",
    "    combined_non_speech_ranges.append(tuple(current_range))\n",
    "\n",
    "# Display non-speech ranges\n",
    "print(\"Non-speech ranges detected:\", combined_non_speech_ranges)\n",
    "\n",
    "# # Convert text to speech and save it as an audio file\n",
    "# caption = \"Your text description about the video goes here.\"\n",
    "# language = 'en'\n",
    "# speech = gTTS(text=caption, lang=language, slow=False)\n",
    "# speech.save(\"caption.mp3\")\n",
    "\n",
    "# Load the audio description\n",
    "caption_audio = AudioFileClip(\"caption.mp3\")\n",
    "caption_duration = caption_audio.duration\n",
    "\n",
    "# Find a suitable non-speech range\n",
    "suitable_range = None\n",
    "for start, end in combined_non_speech_ranges:\n",
    "    if (end - start) >= caption_duration:\n",
    "        suitable_range = (start, end)\n",
    "        break\n",
    "\n",
    "# Insert audio description into the suitable non-speech range\n",
    "if suitable_range:\n",
    "    start_non_speech, end_non_speech = suitable_range\n",
    "    print(f\"Inserting audio description into non-speech part from {start_non_speech} to {end_non_speech} seconds\")\n",
    "\n",
    "    # Trim or extend the caption audio to fit the non-speech part\n",
    "    caption_audio = caption_audio.subclip(0, caption_duration)\n",
    "\n",
    "    # Split the video into three parts: before, non-speech part, and after\n",
    "    video_before = video.subclip(0, start_non_speech)\n",
    "    video_non_speech = video.subclip(start_non_speech, start_non_speech + caption_duration)\n",
    "    video_after = video.subclip(start_non_speech + caption_duration)\n",
    "\n",
    "    # Replace the non-speech part's audio with the caption audio\n",
    "    video_non_speech = video_non_speech.set_audio(caption_audio)\n",
    "\n",
    "    # Concatenate the video parts back together\n",
    "    final_video = concatenate_videoclips([video_before, video_non_speech, video_after])\n",
    "\n",
    "    # Save the final video\n",
    "    final_video.write_videofile(\"final_video_with_caption.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
    "else:\n",
    "    print(\"No suitable non-speech part found in the video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d6614-8f54-495d-9ee5-d28e47fa47ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
